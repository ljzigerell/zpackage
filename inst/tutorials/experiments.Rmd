---
title: "Experiments"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo=FALSE)
```

## Hypotheses

A hypothesis is a claim. The hypothesis that we test is called the **null hypothesis**. Everything not included in the null hypothesis is included in the **alternative hypothesis**. For example:

The null hypothesis: There is no association.<br/>
The alternative hypothesis: There is an association.

The null hypothesis: Participants will not have a gender bias.<br/>
The alternative hypothesis: Participants will have a gender bias.

The null hypothesis: The treatment will reduce gender bias.<br/>
The alternative hypothesis: The treatment will not reduce gender bias.

### Quiz

```{r null-hypothesis}
question("Suppose that the null hypothesis is that a treatment will have no effect. Which of the following would be the alternate hypothesis?",
  answer("The treatment will have no effect."),
  answer("The treatment will have an effect.", correct = TRUE),
  answer("The treatment will have a positive effect."),
  answer("The treatment will have a negative effect."),
  allow_retry=TRUE
)
```


## p-values

Let's introduce an important tool: the **p-value**. The p-value is a measure of the strength of the evidence that an analysis provides against the null hypothesis. 

If an analysis provided no evidence against the null hypothesis, the p-value is 1. The lower the p-value is, the more evidence the analysis provided against the null hypothesis. A p-value of zero would indicate that the analysis provided infinitely strong evidence against the null hypothesis.

### Quiz

```{r pvalue-range}
question("Suppose that we flip a coin 200 times and test the null hypothesis that the coin is fair. Which p-value below is possible for a test of that null hypothesis?",
  answer("-0.10"),
  answer("0"),
  answer("0.10", correct=TRUE, message="p-values range from 1 to 0."),
  answer("200"),
  allow_retry=TRUE
)
```

```{r pvalue-strength}
question("Which p-value below would provide stronger evidence against the null hypothesis?",
  answer("p=0.01", correct=TRUE, message="Lower p-values indicate stronger evidence against the null hypothesis."),
  answer("p=0.50"),
  answer("p=0.99"),
  allow_retry=TRUE
)
```

```{r pvalue01a}
question("Suppose that a coin is flipped 10 times, with 5 heads and 5 tails. What would be the p-value for a test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("1", correct=TRUE, message="No evidence against the null hypotheis."),
  answer("something between 0 and 1"),
  allow_retry=TRUE
)
```

```{r pvalue01b}
question("Suppose that a coin is flipped 10 times, with 4 heads and 6 tails. What would be the p-value for a test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("1"),
  answer("something between 0 and 1", correct=TRUE),
  allow_retry=TRUE
)
```

```{r pvalue01c}
question("Suppose that a coin is flipped 10 times, with 0 heads and 10 tails. What would be the p-value for a test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("1"),
  answer("something between 0 and 1", correct=TRUE, message="There is some evidence against the null hypothesis but not infinitely strong evidence"),
  allow_retry=TRUE
)
```

## Calculating p-values

Suppose that our null hypothesis is that our coin is fair. So we flip the coin four times. Here are the 16 possible outcomes for a fair coin, with each outcome equally likely:

<p style="font-family:'Courier New'">
4 heads: HHHH</br>
3 heads: HHHT  HHTH  HTHH  THHH</br>
2 heads: HHTT  HTHT  HTTH  THHT  THTH  TTHH</br>
1 heads: HTTT  THTT  TTHT  TTTH</br>
0 heads: TTTT</p>

If our coin landed on heads 4 times in 4 flips, the p-value would be the percentage of times that a fair coin would be expected to produce an outcome **at least as extreme** as 4 heads in 4 flips. The "at least as extreme" in this case is at least as far from the expected long-run average for a fair coin of 50% heads and 50% tails. In this example, the only outcomes that are at least as extreme as 4 heads in 4 flips are HHHH and TTTT. So 2 outcomes of 16 possible outcomes produces a p-value of 2/16 or 0.125. This p-value of 0.125 means that, 12.5% of the time, a fair coin flipped 4 times would be expected to be as far from fairness as our HHHH coin was from fairness; the other 87.5% of the time, a fair coin would be expected to be closer to fairness than our HHHH coin was to fairness.

We can use a statistical program called R to calculate this p-value in a binomial test. A **binomial test** is useful for calculating p-values for binary outcomes. The binom.test command below will calculate a p-value for a situation in which there were "x" heads in n" coin flips, in which the probability is "p" for each coin landing on heads. Click the "Run Code" button below:

```{r pvalue-coinbi, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=1}
binom.test(x=4, n=4, p=0.50)
```

### Quiz

Enter and run R code using the R "binom.test()" function to calculate the p-value for the test of the null hypothesis that a coin is fair, for a coin that landed on heads exactly 54 times in 100 flips. You should get a p-value of p=0.4841.

```{r pvalue-coinbi-self, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=1}

```

```{r pvalue34}
question("See above for the 16 outcomes for 4 flips of a fair coin. If the coin landed on heads exactly 3 times in 4 flips, what would be the p-value for the test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("0.125"),
  answer("0.250"),
  answer("0.500"),
  answer("0.625", correct=TRUE, message="There are 10 outcomes at least as extreme as 3 heads in 4 flips, so the p-value is 10/16."),
  answer("0.750"),
  answer("0.875"),
  answer("1"),
  allow_retry=TRUE
)
```

```{r pvalue16}
question("See above for the 16 outcomes for 4 flips of a fair coin. If the coin landed on heads exactly 2 times in 4 flips, what would be the p-value for the test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("0.125"),
  answer("0.250"),
  answer("0.500"),
  answer("0.625"),
  answer("0.750"),
  answer("0.875"),
  answer("1", correct=TRUE, message="All 10 outcomes are at least as extreme as 4 heads in 4 flips, so the p-value is 16/16. Another way to think about it is that, if a coin lands on heads 2 times in 4 flips, there is no evidence against the null hypothesis that the coin is fair."),
  allow_retry=TRUE
)
```

## Hypothesis testing

The basic logic of hypothesis testing is to:

1. Select a null hypothesis.
2. Collect observations to test the null hypothesis.
3. Assess how likely the observed data would be if the null hypothesis were true.
4. If the observed data would be too unlikely to have occurred if the null hypothesis were true, then reject the null hypothesis and accept the alternative hypothesis; otherwise, do not accept or reject the null hypothesis or the alternative hypothesis.

The conventional threshold for rejecting the null hypothesis is p=0.05 in political science. For reference, given a null hypothesis that the coin is fair, the p-value is 0.0625 for a coin that landed on heads 5 times in 5 flips.

---

Suppose that our null hypothesis is that our coin is fair. The coin lands on heads exactly 1 time in 10 flips, so that the p-value is 0.02 for this test of the null hypothesis. Based on this p-value, we would reject the null hypothesis that the coin is fair, and accept the alternative hypothesis that the coin is not fair.

But suppose that the coin landed on heads exactly 2 times in 10 flips, so that the p-value is 0.11 for the test of the null hypothesis that the coin is fair. Based on this p-value, we would not reject or accept the null hypothesis that the coin is fair, and we would not accept or reject the alternative hypothesis that the coin is not fair. The observed data provided some evidence that the coin is unfair, so we would not want to reject the alternative hypothesis that the coin is unfair. But the observed data also did not provide enough evidence for us to reject the null hypothesis that the coin is fair.

### Quiz

```{r pvalue001}
question("Given the conventional threshold for rejecting the null hypothesis in political science, if the p-value is p=0.01 for the test of a null hypothesis, then which, if any, of the following should we do?",
  answer("accept the null hypothesis, and accept the alternative hypothesis"),
  answer("accept the null hypothesis, and reject the alternative hypothesis"),
  answer("reject the null hypothesis, and accept the alternative hypothesis", correct=TRUE, message="The p-value is lower than p=0.05, so we can accept the claim that the coin is unfair."),
  answer("reject the null hypothesis, and reject the alternative hypothesis"),
  answer("none of the above"),
  allow_retry=TRUE
)
```

```{r pvalue049}
question("Given the conventional threshold for rejecting the null hypothesis in political science, if the p-value is p=0.49 for the test of a null hypothesis, then which, if any, of the following should we do?",
  answer("accept the null hypothesis, and accept the alternative hypothesis"),
  answer("accept the null hypothesis, and reject the alternative hypothesis"),
  answer("reject the null hypothesis, and accept the alternative hypothesis"),
  answer("reject the null hypothesis, and reject the alternative hypothesis"),
  answer("none of the above", correct=TRUE, message="The p-value is not lower than p=0.05, so we do not accept or reject any hypotheis."),
  allow_retry=TRUE
)
```

## Confidence intervals

Let's discuss the 95% confidence interval of [0.4286584, 0.5713416] in the output for the binomial test below, for a coin flipped 200 times that landed on heads exactly 100 times. Click "Run Code" to run the binomial test:

```{r pvalue-coinbix, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=1}
binom.test(x=100, n=200, p=0.50)
```

The 95% confidence interval in this case rounds to [0.43, 0.57]. The 0.43 and 0.57 are the ends of the range of the middle 95% of percentage heads that are expected when a fair coin is flipped 200 times. So the bottom end of the 95% confidence interval indicates that, in 200 flips, about 2.5% of coin flips are expected to have less than 43% heads, and the top end of the 95% confidence interval indicates that, in 200 flips, about 2.5% coin flips are expected to be greater than 57% heads.

---

Below is an R simulation to illustrate this. Click "Run Code" to run the code:

```{r pvalue-simul, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=17}

LIST.PCTS <- c()    # Make a blank list
COINS     <- c(0,1) # Make a list of coin outcomes (0=Tails, 1=Heads)

for (i in 1:25000){                                 # Run the bracketed code 10 thousand times
  PCT.HEADS <- sum(sample(COINS,200,replace=T))/200 # Get the percentage heads for a random sample of a fair coin flipped 200 times
  LIST.PCTS <- append(LIST.PCTS, PCT.HEADS)         # Add that percentage heads to the list of percentages
                   }
LO95 <- quantile(LIST.PCTS, 0.025) # Get the low  end of the 95% confidence interval
HI95 <- quantile(LIST.PCTS, 0.975) # Get the high end of the 95% confidence interval

breaks <- hist(LIST.PCTS, breaks=seq(0.3255,0.6705,0.005), plot=FALSE)$breaks # Get a list of breaks for the histogram
colors <- rep("gray80", length(breaks))                                       # Start with all colors in the histogram as "gray80"
colors[(breaks < LO95) | (breaks > HI95)] <- "blue"                           # Change the colors to "blue" if the histogram bar is 
                                                                              # lower than the low end of the 95% confidence interval or 
                                                                              # higher than the high end of the 95% confidence interval
hist(LIST.PCTS, breaks=seq(0.3255,0.6755,0.005), ylim=c(0,1800), col=colors)  # Plot the histogram

```

---

Let's run another simulation, this time illustrating the fact that, if drawing random samples from a set of data, 95% of the 95% confidence intervals for the mean of the data are expected to contain the true mean. In the simulation, the red lines are 95% confidence intervals that do not contain the true mean, and the gray lines are 95% confidence intervals that do contain the true mean. Click "Run Code" to run the simulation:

```{r ci-simul95, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=31, fig.width=9, fig.height=5}

Y         <- 100000     # The top number in our dataset.
DATA      <- 0:Y        # This gets a dataset that ranges from 0 to 100,000.
TRUE.MEAN <- mean(DATA) # This is the true mean of the dataset.
RUNS      <- 2000       # This is the number of runs our simulation will have.
COUNTER   <- 0          # This is our counter.

plot(NULL, xlim=c(0,RUNS), ylim=c(0,Y), yaxt="n", xlab="Run #", ylab="95% CIs") # Sets up the plot
axis(side=2, at=seq(0,2000,500), labels=TRUE)                                   # Sets up the y-axis

for (i in 1:RUNS){
    SAMPLE.SIZE <- 10                                   # Set to sample 10.
    SAMPLE      <- sample(DATA, SAMPLE.SIZE, replace=F) # Draw a sample of 10 from DATA.
    SAMPLE.MEAN <- mean(SAMPLE)                         # Get the mean of that sample.
    SD          <- sd(SAMPLE)                           # Get the standard deviation of the sample.
    T.STATISTIC <- qt(0.975, df=SAMPLE.SIZE-1)          # This is used to calculate the confidence interval.
    CI.WIDTH    <- T.STATISTIC * SD/sqrt(SAMPLE.SIZE)   # This is the width of the confidence interval.
    CI.LO       <- SAMPLE.MEAN - CI.WIDTH               # This is the low end of the confidence interval.
    CI.HI       <- SAMPLE.MEAN + CI.WIDTH               # This is the high end of the confidence interval.
    if((TRUE.MEAN >= CI.LO) & (TRUE.MEAN <= CI.HI)){    # This "if" sequence plots a gray line for
        COUNTER <- COUNTER + 1                          # 95% confidence intervals that contain  
        segments(i, CI.LO, i, CI.HI, col="gray")        # the true mean, and plots a red line for
              } else {                                  # 95% confidence intervals that does not
        segments(i, CI.LO, i, CI.HI, col="firebrick4")  # contain the true mean.
              }
    }

abline(h=TRUE.MEAN, col="dodger blue") # Plots the true mean

print("Percentage of 95% confidence intervals that contain the true mean:")
100 * COUNTER / RUNS
```

---

The confidence interval above was a 95% confidence interval. But other confidence intervals are possible. The higher then percentage for the confidence interval, the wider the confidence interval. For example, a 99% confidence interval must contain the true mean 99% of the time, so the number of means in a 99% confidence interval must be larger than for a 95% confidence interval.

Here is a simulation for 50% confidence intervals, illustrating that only about half of 50% confidence intervals contain the true mean. Click "Run Code" to run the simulation:

```{r ci-simul99, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=30, fig.width=9, fig.height=5}

Y         <- 100000
DATA      <- 0:Y
TRUE.MEAN <- mean(DATA)
RUNS      <- 2000
COUNTER   <- 0

plot(NULL, xlim=c(0,RUNS), ylim=c(0,Y), yaxt="n", xlab="Run #", ylab="95% CIs")
axis(side=2, at=seq(0,2000,500), labels=TRUE)

for (i in 1:RUNS){
    SAMPLE.SIZE <- 10
    SAMPLE      <- sample(DATA, SAMPLE.SIZE, replace=F)
    SAMPLE.MEAN <- mean(SAMPLE)
    SD          <- sd(SAMPLE)
    T.STATISTIC <- qt(0.750, df=SAMPLE.SIZE-1)         # This is the line that changed for 50% confidence intervals.
    CI.WIDTH    <- T.STATISTIC * SD/sqrt(SAMPLE.SIZE)
    CI.LO       <- SAMPLE.MEAN - CI.WIDTH
    CI.HI       <- SAMPLE.MEAN + CI.WIDTH
    if((TRUE.MEAN >= CI.LO) & (TRUE.MEAN <= CI.HI)){
        COUNTER <- COUNTER + 1 
        segments(i, CI.LO, i, CI.HI, col="gray")
              } else {
        segments(i, CI.LO, i, CI.HI, col="firebrick4")
              }
    }

abline(h=TRUE.MEAN, col="dodger blue")

print("Percentage of 50% confidence intervals that contain the true mean:")
100 * COUNTER / RUNS
```

## Randomized experiments

---

### Random assignment error

Suppose that we have a group of male participants and female participants. We randomly assign each participant a number from 0 to 100. We then calculate the mean of the numbers among the male participants and the mean of the numbers among the female participants. There is no reason to expect the mean to be higher among male participants or lower among male participants, compared to the mean among female participants. But if the mean among male participants does differ than the mean among female participants, we call that **random assignment error**.

---

Typically, random assignment error decreases as sample size increases. Let's illustrate that below, with a plot in which the x-axis is the sample size of a random draw of numbers from 0 to 100, and the y-axis is the mean of the numbers in that draw. The plot of points is expected to get closer to the true mean of 50 as the sample size increases.

```{r lawlarge, echo=TRUE, eval=TRUE, exercise=TRUE, fig.width=9, fig.height=8}

plot(NULL, xlim=c(0,800), ylim=c(20,80), xlab="Sample size", ylab="Mean")
abline(h=50, col="dodgerblue")
for (i in 1:800){
    SAMPLE <- mean(sample(0:100, i, replace=T))
    points(i, SAMPLE, pch=16, cex=0.5)
    }
```

---

### The logic of a randomized experiment

The typical political science randomized experiment works like this:
1. Gather a set of participants.
2. Randomly assign each participant to a group.
3. Treat the groups differently.
4. Measure something about the groups.

The observed difference between groups in Step 4 must be due to some combination of:
*	the treatment difference in Step 3 causing the observed difference
*	random assignment error in Step 2 causing the observed difference

And if the p-value is less than 0.05 for a test of the null hypothesis that the mean of one group equals the mean of the other group, then we can eliminate random assignment error as a likely explanation for the observed difference.

---

### Sample analysis of data from a randomized experiment

Let's analyze data from the experiment in the Rice et al. 2021 Journal of Politics article "Same as it ever was? The impact of racial resentment on White juror decision-making". In this experiment, participants were randomly assigned to read a story about a basketball player who was accused of a crime and had a White-sounding name of Bradley Schwartz, or to read the same story but in which the basketball player accused of a crime had a Black-sounding name of Jamal Gaines. 

One of the outcomes of the experiment was an indication of whether the basketball player was guilty. Among the 301 White participants assigned to the "Bradley" treatment, 157 reported a guilty judgment, 143 reported a not guilty judgment, and 1 did not report a judgment; among the 350 White participants assigned to the "Jamal" treatment, 159 reported a guilty judgment, 190 reported a not guilty judgment, and 1 did not report a judgment.

So, among White participants, 157 of 301 reported a guilty judgment for Bradley, and 159 of 350 reported a guilty judgment for Jamal. Let's test the null hypothesis that the percentage that reported a guilty judgment for Bradley equals the percentage that reported a guilty judgment for Jamal.

For this, we can use a Fisher's exact test. Click "Run Code" to run the Fisher's exact test:

```{r rice-fisher, echo=TRUE, exercise=TRUE}
RICE <- matrix(c(157,144,159,191), nrow=2, 
          dimnames=list(Verdict=c("Guilty","Other"),Treatment=c("Bradley","Jamal")))

print(RICE)
fisher.test(RICE)
```

Below is a simulation that should produce an estimate of the p-value close to the 0.09861 p-value from the Fisher's exact test:

```{r rice-simul-guilty, echo=TRUE, eval=TRUE, exercise=TRUE, exercise.lines=15}

DIFF.OBSRVD <- abs(157/301 - 159/350)
COMBINED    <- c(rep.int(1,316), rep.int(0,335))
COUNTER     <- 0
RUNS        <- 100000

for (i in 1:RUNS){
   BRADLEY     <- sum(sample(COMBINED, 301, replace=FALSE))
   JAMAL       <- 316 - BRADLEY
   DIFF.RANDOM <- abs(BRADLEY/301 - JAMAL/350)
   if (DIFF.RANDOM >= DIFF.OBSRVD) {
     COUNTER   <- COUNTER + 1
                                  }
                  }
COUNTER / RUNS
```

---

For another outcome measured in the Rice et al experiment after the story was presented, participants were asked to recommend a sentence length between zero months and 60 months for the basketball player. 

Below is a list of these sentence lengths:

```{r rice-data, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(tidyverse)

RICE <- tribble(
~TREAT,~LENGTH,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,
"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 0,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,
"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 1,"Bradley", 2,"Bradley", 2,
"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,
"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,
"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 2,"Bradley", 3,"Bradley", 3,
"Bradley", 3,"Bradley", 3,"Bradley", 3,"Bradley", 3,"Bradley", 3,"Bradley", 3,"Bradley", 3,
"Bradley", 3,"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,
"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,"Bradley", 4,
"Bradley", 4,"Bradley", 4,"Bradley", 5,"Bradley", 5,"Bradley", 5,"Bradley", 5,"Bradley", 5,
"Bradley", 5,"Bradley", 5,"Bradley", 5,"Bradley", 5,"Bradley", 5,"Bradley", 5,"Bradley", 6,
"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,
"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,
"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,"Bradley", 6,
"Bradley", 7,"Bradley", 7,"Bradley", 7,"Bradley", 7,"Bradley", 8,"Bradley", 8,"Bradley", 8,
"Bradley", 8,"Bradley", 9,"Bradley", 9,"Bradley", 9,"Bradley", 9,"Bradley", 9,"Bradley", 9,
"Bradley", 9,"Bradley", 9,"Bradley", 9,"Bradley", 9,"Bradley", 9,"Bradley",10,"Bradley",10,
"Bradley",10,"Bradley",10,"Bradley",10,"Bradley",12,"Bradley",12,"Bradley",12,"Bradley",12,
"Bradley",12,"Bradley",12,"Bradley",12,"Bradley",13,"Bradley",13,"Bradley",14,"Bradley",15,
"Bradley",15,"Bradley",15,"Bradley",15,"Bradley",15,"Bradley",15,"Bradley",15,"Bradley",15,
"Bradley",15,"Bradley",16,"Bradley",16,"Bradley",18,"Bradley",18,"Bradley",19,"Bradley",19,
"Bradley",23,"Bradley",24,"Bradley",24,"Bradley",24,"Bradley",24,"Bradley",24,"Bradley",27,
"Bradley",28,"Bradley",28,"Bradley",29,"Bradley",30,"Bradley",30,"Bradley",30,"Bradley",30,
"Bradley",30,"Bradley",30,"Bradley",30,"Bradley",30,"Bradley",30,"Bradley",30,"Bradley",30,
"Bradley",31,"Bradley",31,"Bradley",31,"Bradley",36,"Bradley",36,"Bradley",37,"Bradley",41,
"Bradley",41,"Bradley",45,"Bradley",45,"Bradley",46,"Bradley",46,"Bradley",48,"Bradley",48,
"Bradley",49,"Bradley",49,"Bradley",59,"Bradley",59,"Bradley",60,"Bradley",60,"Bradley",60,
"Bradley",60,"Bradley",60,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,
"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 0,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,
"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,
"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,
"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,
"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 1,"Jamal", 2,"Jamal", 2,
"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,
"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,
"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 2,
"Jamal", 2,"Jamal", 2,"Jamal", 2,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,
"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 3,"Jamal", 4,
"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,
"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 4,"Jamal", 5,"Jamal", 5,"Jamal", 5,"Jamal", 5,"Jamal", 5,
"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,
"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,
"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 6,"Jamal", 7,
"Jamal", 7,"Jamal", 7,"Jamal", 7,"Jamal", 7,"Jamal", 8,"Jamal", 8,"Jamal", 8,"Jamal", 8,"Jamal", 8,
"Jamal", 9,"Jamal", 9,"Jamal", 9,"Jamal",10,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,
"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,"Jamal",12,
"Jamal",13,"Jamal",14,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,
"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,"Jamal",15,
"Jamal",16,"Jamal",16,"Jamal",17,"Jamal",17,"Jamal",17,"Jamal",18,"Jamal",20,"Jamal",24,"Jamal",25,
"Jamal",28,"Jamal",30,"Jamal",30,"Jamal",30,"Jamal",30,"Jamal",30,"Jamal",30,"Jamal",30,"Jamal",31,
"Jamal",31,"Jamal",31,"Jamal",31,"Jamal",31,"Jamal",32,"Jamal",32,"Jamal",33,"Jamal",34,"Jamal",35,
"Jamal",36,"Jamal",36,"Jamal",39,"Jamal",42,"Jamal",43,"Jamal",44,"Jamal",45,"Jamal",45,"Jamal",46,
"Jamal",49,"Jamal",52,"Jamal",59,"Jamal",59,"Jamal",60,"Jamal",60,"Jamal",60,"Jamal",60,"Jamal",60)
```

If certain assumptions are met, we can use a t-test to test the null hypothesis that the mean of one group equals the mean of another group.

```{r rice-ttest, echo=TRUE, eval=TRUE, cache=TRUE, dependson="rice-data"}

t.test(LENGTH ~ TREAT, data=RICE)
```

Let's check the p-value above against the p-value from a simulation. 

```{r rice-simul-length, echo=TRUE, eval=TRUE, cache=TRUE, dependson="rice-data"}

BRADLEY.MEAN <- mean(RICE$LENGTH[RICE$TREAT=="Bradley"])
JAMAL.MEAN   <- mean(RICE$LENGTH[RICE$TREAT=="Jamal"])
DIFF.OBSRVD  <- abs(BRADLEY.MEAN - JAMAL.MEAN)
DIFF.OBSRVD    

COUNTER       <- 0
RUNS          <- 100000
LIST.RANDOM   <- c()

LENGTH.TOTAL  <- sum(RICE$LENGTH)
LENGTH.TOTAL

for (i in 1:RUNS){
   BRADLEY.RANDOM <- sum(sample(RICE$LENGTH, 296, replace=F))
   JAMAL.RANDOM   <- LENGTH.TOTAL - BRADLEY.RANDOM 
   DIFF.RANDOM    <- abs(BRADLEY.RANDOM/296 - JAMAL.RANDOM/342)
   LIST.RANDOM    <- append(LIST.RANDOM, DIFF.RANDOM)
   if (DIFF.RANDOM > DIFF.OBSRVD) {
     COUNTER   <- COUNTER + 1
                                  }
                  }

hist(LIST.RANDOM)
abline(v=c(DIFF.OBSRVD, -DIFF.OBSRVD), col="blue")
COUNTER / RUNS
```

## Review questions

```{r review1}
question("Which p-value below would provide stronger evidence against the null hypothesis?",
  answer("p=0.03", correct=TRUE, message="Lower p-values indicate stronger evidence against the null hypothesis."),
  answer("p=0.97"),
  allow_retry=TRUE
)
```

```{r review2}
question("Suppose that a coin is flipped 100 times, with 100 heads and 0 tails. What would be the p-value for a test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("1"),
  answer("something between 0 and 1", correct=TRUE, message="There is some evidence against the null hypothesis but not infinitely strong evidence"),
  allow_retry=TRUE
)
```

```{r review3}
question("Suppose that a coin is flipped 100 times, with 50 heads and 50 tails. What would be the p-value for a test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("1", correct=TRUE, message="No evidence against the null hypotheis."),
  answer("something between 0 and 1"),
  allow_retry=TRUE
)
```

```{r review4}
question("Suppose that a coin is flipped 99 times, with 49 heads and 50 tails. What would be the p-value for a test of the null hypothesis that the coin is fair?",
  answer("0"),
  answer("1", correct=TRUE, message="No evidence against the null hypotheis. This is tricky, but a fair coin flipped an odd number of times cannot do anything more fair than a heads/tails difference of 1."),
  answer("something between 0 and 1"),
  allow_retry=TRUE
)
```

```{r review5}
question("If the p-value is p=0.0001 for a test of the null hypothesis that a coin is fair, is that sufficient evidence to support a claim that the bias of the coin is large?",
  answer("Yes"),
  answer("No", correct=TRUE, message="A p-value indicates only the strength of evidence against a null hypothesis, which in this case is the strength of evidence that the coin is not fair. The p-value does not indicate anythign about *how* unfair the coin is."),
  allow_retry=TRUE
)
```

```{r review6}
question("Suppose that a researcher conducted a randomized experiment and then compared the mean response from participants in the control to the mean response from participants in the treatment. The p-value was p=0.01 for a test of the null hypothesis that these means equal each other. Based on this p-value, the researcher should conclude that ___.",
  answer("the treatment had an effect", correct=TRUE, message="The p-value is under the p=0.05 threshold conventional is political science."),
  answer("the treatment did not have an effect"), 
  answer("there is not enough evidence to conclude that the treatment had an effect"),
  allow_retry=TRUE
)
```

```{r review7}
question("Suppose that a researcher conducted a randomized experiment and then compared the mean response from participants in the control to the mean response from participants in the treatment. The p-value was p=0.49 for a test of the null hypothesis that these means equal each other. Based on this p-value, the researcher should conclude that ___.",
  answer("the treatment had an effect"),
  answer("the treatment did not have an effect"), 
  answer("there is not enough evidence to conclude that the treatment had an effect", correct=TRUE, message="We merely don't have enough evidence to reject the null hypothesis; but that does not mean that we accept the null hypothesis."),
  allow_retry=TRUE
)
```

```{r review8}
question("Suppose that we accurately measure the height of 50 randomly selected students at Faber College. The 95% confidence interval for the mean height is [5.4, 5.8]. If we randomly select another 50 students at Faber College and accurately measure these students' heights, add these heights to the original 50 heights, and recalculate a 95% confidence interval for the mean height of these 100 students at Faber College, that 95% confidence interval would be expected to be ___.",
  answer("thinner than [5.4, 5.8]", correct=TRUE, message="More evidence about the mean height, so we will close in on the true mean."),
  answer("wider than [5.4, 5.8]"), 
  answer("the same width as [5.4, 5.8]"),
  allow_retry=TRUE
)
```

```{r review9}
question("Suppose that we accurately measure the height of 50 randomly selected students at Faber College. The 95% confidence interval for the mean height is [5.4, 5.8]. The 99% confidence interval for that mean height would be expected to be ___.",
  answer("thinner than [5.4, 5.8]"),
  answer("wider than [5.4, 5.8]", correct=TRUE, message="Higher percentages for a confidence intervals mean than we need to include more numbers in the confidence intervals."), 
  answer("the same width as [5.4, 5.8]"),
  allow_retry=TRUE
)
```
